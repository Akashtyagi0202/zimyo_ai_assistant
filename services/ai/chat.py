# langchain_chat.py
"""
Optimized LLM interaction module using direct OpenAI SDK for performance.

Performance: 200-300ms faster than LangChain wrapper for simple LLM calls.
Note: For advanced features (chains, agents, tools), see ai_agent.py and langchain_tools.py
"""
import os
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

# LLM Provider Configuration
# Set LLM_PROVIDER in .env to choose: "gemini" (default), "deepseek", or "openai"
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "gemini").lower()

# Gemini configuration (Google AI)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")  # or gemini-1.5-pro

# DeepSeek configuration
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"
DEEPSEEK_MODEL = "deepseek-chat"

# OpenAI configuration (fallback)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")  # or gpt-4

# ---------------------- PROMPTS ----------------------
Employees_Chat = """
You are a very professional HR policy analyst who will receive:

1) Details of an employee, having information like the number of leaves, department that they work in, and more.
2) A query asked by the employee.
3) Policy Content that contain HR policies of the organization applicable to the employee.

You must answer the query of the employee by searching for relevant information within the documents. You must cross verify the information with the details of the employee.
Consider the employee data and the information from the document before answering the query.

If the relevant information is not found in the Policy, or you have not received the correct employee information, you must simply answer "My knowledge base doesn't include that information, please ask a different query, or reword your current query".

Only answer the query if you have all the necessary information, and when you do answer, be elaborate.

Provide as much relevant information from the documents as possible.

Your response should strictly follow the following format:

* Response: The answer to the query
* Reference: The Title of the Policy used as reference for the answer OR just Employee details.
"""

HR_Chat = """
You are an extremely professional content creator for an HR team. You will create drafts of E-mails, Job Descriptions, Corporate Policies and more. You must always use a professional and corporate tone while you write these documents.
If the query given to you is not related to HR related topics, simply respond that "That is not in my knowledge base."

You will receive the employee information, and contents of relevant corporate policies as well. If there is any query that asks for a personalized document, refer to this information and create the content.

You must respond to every part of the user's query, and be elaborate in your answers.

Your output strictly follows the following format:

Content: complete content of what the user query requested.
Description: What was asked in the query. Only select one from the following options:"EMAIL", "JOB_DESCRIPTION", "POLICY", "NONE".
"""

system_instructions = {"employee": Employees_Chat, "manager": HR_Chat}

# ---------------------- FUNCTIONS ----------------------

# Create singleton clients for better performance
_gemini_client = None
_openai_client = None

def get_gemini_client():
    """Get Google Gemini client"""
    global _gemini_client
    if _gemini_client is None:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        _gemini_client = genai
        print(f"ðŸ¤– Using Google Gemini API (Model: {GEMINI_MODEL})")
    return _gemini_client

def get_openai_compatible_client() -> OpenAI:
    """
    Returns a singleton client for OpenAI-compatible APIs (DeepSeek, OpenAI)

    Using singleton pattern for:
    - Connection pooling (reuses HTTP connections)
    - Better performance (no client creation overhead per request)
    - Resource efficiency
    """
    global _openai_client
    if _openai_client is None:
        if LLM_PROVIDER == "openai":
            # Use OpenAI
            _openai_client = OpenAI(api_key=OPENAI_API_KEY)
            print(f"ðŸ¤– Using OpenAI API (Model: {OPENAI_MODEL})")
        elif LLM_PROVIDER == "deepseek":
            # Use DeepSeek
            _openai_client = OpenAI(
                api_key=DEEPSEEK_API_KEY,
                base_url=DEEPSEEK_BASE_URL
            )
            print(f"ðŸ¤– Using DeepSeek API (Model: {DEEPSEEK_MODEL})")
    return _openai_client

def get_chat_response(role: str, prompt: str) -> str:
    """
    Sends a message to LLM API (Gemini, DeepSeek, or OpenAI) and returns the response text.

    Supports:
    - Gemini (Google AI) - Default, free tier available
    - DeepSeek - Very cheap alternative
    - OpenAI - Fallback option

    Args:
        role: User role ("employee" or "manager") - determines system prompt
        prompt: User's query/message

    Returns:
        AI-generated response text
    """
    try:
        # Get system prompt based on role
        system_prompt = system_instructions.get(role, Employees_Chat)

        if LLM_PROVIDER == "gemini":
            # Use Gemini
            genai = get_gemini_client()
            model = genai.GenerativeModel(GEMINI_MODEL)

            # Combine system prompt with user prompt for Gemini
            full_prompt = f"{system_prompt}\n\nUser Query: {prompt}"

            response = model.generate_content(full_prompt)
            return response.text

        else:
            # Use OpenAI-compatible APIs (DeepSeek, OpenAI)
            client = get_openai_compatible_client()

            # Choose model based on provider
            model = OPENAI_MODEL if LLM_PROVIDER == "openai" else DEEPSEEK_MODEL

            # Direct API call
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2048
            )

            return response.choices[0].message.content

    except Exception as e:
        provider_name = {
            "gemini": "Gemini",
            "openai": "OpenAI",
            "deepseek": "DeepSeek"
        }.get(LLM_PROVIDER, "LLM")

        return f"Error getting response from {provider_name}: {str(e)}"

def create_description(prompt: str) -> str:
    """
    Generate a short 3-5 word description for the given prompt using LLM.

    Args:
        prompt: User query to generate description for

    Returns:
        Short 3-5 word description
    """
    try:
        if LLM_PROVIDER == "gemini":
            # Use Gemini
            genai = get_gemini_client()
            model = genai.GenerativeModel(GEMINI_MODEL)

            instruction = "You must generate a short 3-5 word prompt for a query given by a user."
            full_prompt = f"{instruction}\n\nUser Query: {prompt}"

            response = model.generate_content(full_prompt)
            return response.text.strip()

        else:
            # Use OpenAI-compatible APIs
            client = get_openai_compatible_client()

            # Choose model based on provider
            model = OPENAI_MODEL if LLM_PROVIDER == "openai" else DEEPSEEK_MODEL

            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You must generate a short 3-5 word prompt for a query given by a user."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=50
            )

            return response.choices[0].message.content.strip()

    except Exception as e:
        return f"Error: {str(e)}"